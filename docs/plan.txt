1. 사전 준비 단계

  - 쇼핑몰 목록 정리 (malls.json 또는 malls-clean.txt)
  - 작업 상태 추적 시스템 구축 (어떤 몰이 완료/실패/대기 중인지)
  - 필요한 도구 설치 확인 (puppeteer, axios, cheerio 등)

  2. 분석 단계 (Analyze)

  각 쇼핑몰의 구조를 파악:
  - 제품 카테고리 구조
  - URL 패턴
  - 페이지네이션 방식
  - 동적 로딩 여부 (JavaScript 렌더링 필요 여부)
  - 제품 데이터 위치 (HTML 구조)

  3. 스크래퍼 생성 단계 (Generate Scraper)

  분석 결과를 바탕으로 각 몰에 맞는 스크래퍼 생성:
  - 정적 사이트: axios + cheerio 사용
  - 동적 사이트: puppeteer 사용
  - 카테고리별 순회 로직
  - 페이지네이션 처리
  - 에러 핸들링

  4. 스크래핑 실행 단계 (Execute)

  생성된 스크래퍼를 실행하여 제품 데이터 수집:
  - 각 스크래퍼 실행
  - 제품 데이터를 JSON 파일로 저장
  - 실행 결과 로깅

  5. 데이터 검증 단계 (Validate)

  수집된 데이터의 품질 확인:
  - 필수 필드 존재 여부 (id, title, price, url)
  - 중복 제품 확인
  - 데이터 형식 검증

  6. 데이터 등록 단계 (Register)

  검증된 데이터를 products.json에 통합:
  - 기존 제품과 중복 확인
  - 새 제품 추가
  - 백업 생성

  7. 분할 실행 전략

  # 방법 1: 지역별 분할
  - 서울/경기 몰들
  - 강원도 몰들
  - 충청도 몰들
  - 전라도 몰들
  - 경상도 몰들
  - 제주도 몰들

  # 방법 2: 배치 분할 (10-15개씩)
  - Batch 1: mall_1 ~ mall_15
  - Batch 2: mall_16 ~ mall_30
  - ...

  # 방법 3: 기술별 분할
  - 정적 사이트 (axios/cheerio)
  - 동적 사이트 (puppeteer)

  8. 작업 추적 파일 구조

  {
    "malls": {
      "mall_1": {
        "name": "온서울마켓",
        "status": "pending|analyzing|scraping|completed|failed",
        "analyzer_created": false,
        "scraper_created": false,
        "products_scraped": 0,
        "products_registered": 0,
        "last_updated": "2024-01-01T12:00:00Z",
        "error": null
      }
    }
  }

  9. 자동화 스크립트 예시

  # process-mall-batch.sh
  #!/bin/bash

  BATCH_SIZE=10
  START_INDEX=$1
  END_INDEX=$((START_INDEX + BATCH_SIZE))

  for i in $(seq $START_INDEX $END_INDEX); do
    mall_info=$(sed -n "${i}p" malls-clean.txt)

    # 1. Analyze
    claude --allowedTools "Bash(*),Read(*),WebFetch(*)" \
      -p "Analyze: $mall_info" < analyze-template.txt

    # 2. Generate scraper
    claude --allowedTools "Bash(*),Edit(*),Read(*),WebFetch(*)" \
      -p "Create scraper: $mall_info" < scraper-template.txt

    # 3. Execute scraper
    npx tsx scripts/scrape-*.ts

    # 4. Register products
    node register-products.js

    # 5. Update tracking file
    node update-status.js "$mall_info" "completed"
  done

  이렇게 분할하면 실패한 부분만 재실행할 수 있고, 진행 상황을 추적할 수 있으며, 서버 부하도 분산시킬 수 있습니다.